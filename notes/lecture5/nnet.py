## History

# 1960 -> Rosenblatt -> Perceptron
# f(x) = 1 if Wx + b >0
#        0 otherwise

# Multi layer Perceptron

# All in hardware

# 1970 -> Quiet

# 1986 -> Rumelhart -> Backpropagation
# Mulitlayer networks
# Not good training

# 2006 -> Hinton
# 10 Layer Network that trains proporly
# Restricted Boltzman machine
# Sigmoid Function as activation (Not great) - deep learning

# 2010 - 2012 : Really working well
# GMM HMM
# Microsoft - Speech Recognition
# 2012 - Visual Recognition Krizhevsky, Hinton, Sutskever


# Activation functions - why isn't signmoid a good function
# why is it bad if the weights are all positive
# LeCun's paper on why tanH is better than sigmoid
# Addiitonal Read : http://stats.stackexchange.com/questions/142348/tanh-vs-sigmoid-in-neural-net





